---
title: "Exercise 1"
author: "Jack Cunningham, Ali Fazl, and Ken Noddings"
date: "2023-01-30"
output: md_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse)
library(ggplot2)
library(ggmap)
library(broom)
library(knitr)
library(haven)
library(rsample)
library(caret)
library(modelr)
library(parallel)
library(ipred)
library(foreach)
library(scales)
ABIA = read.csv("/Users/jack/Documents/GitHub/Data-Mining-Statistical-Learning/Exercise 1/ABIA.csv")
olympics = read.csv("/Users/jack/Documents/GitHub/Data-Mining-Statistical-Learning/Exercise 1/olympics_top20.csv")
sclass = read.csv("/Users/jack/Documents/GitHub/Data-Mining-Statistical-Learning/Exercise 1/sclass.csv")
```

## Exercise 1: Data visualization - flights at ABIA

*Your task is to create a figure, or set of related figures, that tell an interesting story about flights into and out of Austin. You should annotate your figure(s), of course, but strive to make them as easy to understand as possible at a quick glance.*

``` {r 1}
ABIA = ABIA %>%
  mutate(CRSDepTime2 = (CRSDepTime)^2)

lm1 = lm(LateAircraftDelay ~ CRSDepTime, data=ABIA)
summary(lm1)

ggplot(data = ABIA, aes(x=CRSDepTime, y=LateAircraftDelay)) + 
  geom_jitter(width = 0.2) + 
  stat_smooth(method = "lm1", se = FALSE)
```

*Notes go here*

&nbsp;

## Exercise 2: Wrangling the Olympics

### A) What is the 95th percentile of heights for female competitors across all Athletics events (i.e., track and field)?

```{r 2a}


```

&nbsp;

### B) Which single women's event had the greatest variability in competitor's heights across the entire history of the Olympics, as measured by the standard deviation?

```{r 2b}


```

&nbsp;

### C) How has the average age of Olympic swimmers changed over time? Does the trend look different for male swimmers relative to female swimmers? Create a data frame that can allow you to visualize these trends over time, then plot the data with a line graph with separate lines for male and female competitors. Give the plot an informative caption answering the two questions just posed.

```{r 2c}


```

&nbsp;

## Exercise 3

*Your goal is to use K-nearest neighbors to build a predictive model for price, given mileage, separately for each of two trim levels: 350 and 65 AMG...That is, you'll be treating the 350's and the 65 AMG's as two separate data sets.*

*For each of these two trim levels:*

*1. Split the data into a training and a testing set.*
*2. Run K-nearest-neighbors, for many different values of K, starting at K=2 and going as high as you need to. For each value of K, fit the model to the training set and make predictions on your test set.*
*3. Calculate the out-of-sample root mean-squared error (RMSE) for each value of K.*

*For each trim, make a plot of RMSE versus K, so that we can see where it bottoms out. Then for the optimal value of K, show a plot of the fitted model, i.e. predictions vs. x. (Again, separately for each of the two trim levels.)*

*Which trim yields a larger optimal value of K? Why do you think this is?*

&nbsp;

```{r 3.1}
#Create data frames, plot RMSE by k-value

######trim==65AMG
data_65AMG=sclass%>%
  filter(trim=='65 AMG')
group_split65 = initial_split(data_65AMG, prop=0.8)
group_train65 = training(group_split65)
group_test65 = testing(group_split65)


k_65 = c(2, 4, 6, 8, 10, 15, 20, 25, 30, 35, 40, 45,
           50, 60, 70, 80, 90, 100, 120, 140, 160, 180, 200, 233)

rmse_65 = foreach(kk=k_65, .combine='rbind') %do% {
  knn_65 = knnreg(price~mileage, data=group_train65, k=kk)
  modelr::rmse(knn_65, data=group_test65)
}

df65 <- data.frame(x = k_65, y = rmse_65)
ggplot(df65) +
  geom_point(aes(x = k_65, y = rmse_65)) +
  labs(x="K value",
       y="Root Mean Squared Error",
       title="RMSE by K-value, 65 AMG Trim") 

######trim==350

data_350=sclass%>%
  filter(trim=='350')
group_split350 = initial_split(data_350, prop=0.8)
group_train350 = training(group_split350)
group_test350 = testing(group_split350)
k_350 = c(2, 4, 6, 8, 10, 15, 20, 25, 30, 35, 40, 45,
         50, 55, 60, 65, 70, 80, 90, 100, 120, 140, 160, 180, 200, 250, 300)


rmse_350 = foreach(kk =k_350, .combine='rbind') %do% {
  knn_350 = knnreg(price~mileage, data=group_train350, k=kk)
  modelr::rmse(knn_350, data=group_test350)
}

df350 <- data.frame(x = k_350, y = rmse_350)
ggplot(df350) +
  geom_point(aes(x = k_350, y = rmse_350)) + 
  labs(x="K value",
       y="Root Mean Squared Error",
       title="RMSE by K-value, 65 AMG Trim") 

k350_35 = knnreg(price~mileage, data=group_train350, k=35)
  modelr::rmse(k350_35, data=group_test350)
  
k65_20 = knnreg(price~mileage, data=group_train65, k=20)
  modelr::rmse(k65_20, data=group_test65)
```


```{r 3.2}
#Plot fitted models

group_test65 = group_test65 %>%
  mutate(price_pred = predict(k65_20, group_test65))

p_test65 = ggplot(data = group_test65) + 
  geom_point(mapping = aes(x = mileage, y = price), alpha=0.5) +
  scale_y_continuous(name="Price ($)", labels = comma) +
  scale_x_continuous(name="Mileage", labels = comma) + 
  labs(title="Fitted Model for 65 AMG Trim with K = 20") +
  geom_line(aes(x = mileage, y = price_pred), color='red', linewidth=1.5)
p_test65

group_test350 = group_test350 %>%
  mutate(price_pred = predict(k350_35, group_test350))

p_test350 = ggplot(data = group_test350) + 
  geom_point(mapping = aes(x = mileage, y = price), alpha=0.5) +
  scale_y_continuous(name="Price ($)", labels = comma) +
  scale_x_continuous(name="Mileage", labels = comma) + 
  labs(title="Fitted Model for 350 Trim with K = 35") +
  geom_line(aes(x = mileage, y = price_pred), color='red', linewidth=1.5)
p_test350
```

&nbsp;

RMSE is minimized for the 350 trim at K = 35, with an RMSE value of 9,290.

For the 65 AMG trim, RMSE is minimized at K = 20, with a value of 20,941.

The 350 trim yields a larger optimal value of K, 35 vs. 20. This is likely due to a larger sample size. The dataset contains 417 vehicles with the 350 trim, but only 292 vehicles with the 65 AMG trim. With more data points nearby, we can afford a higher K-value, since the bias is offset by the higher concentration of data points in each "neighborhood." 




