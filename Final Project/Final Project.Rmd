---
title: "Final Project"
author: "Jack Cunningham & Ali Fazl"
date: "`r Sys.Date()`"
output: md_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(gamlr)
library(tidytext)
library(parallel)
library(dplyr)
library(knitr)
library(glmnet)
library(tm)
library(ggplot2)
library(igraph)
library(arules)
library(arulesViz)
library(mosaic)
library(rpart)
library(rpart.plot)
library(rsample) 
library(randomForest)
library(lubridate)
library(caret)
library(Matrix)
library(modelr)
library(gbm)
library(pdp)
library(ggmap)
library(tidycensus)
library(MazamaLocationUtils)
library(ggcorrplot)
library(tigris)
library(fuzzyjoin)
library(sf)
library(data.table)
num_cores <- detectCores()
NY_Spring <- read.csv("/Users/jack/Documents/GitHub/Data-Mining-Statistical-Learning/Final Project/listings 2.csv")
NY_Winter <- read.csv("/Users/jack/Documents/GitHub/Data-Mining-Statistical-Learning/Final Project/dec22.csv")
NY_Summer <- read.csv("/Users/jack/Documents/GitHub/Data-Mining-Statistical-Learning/Final Project/june22.csv")
NY_Fall <- read.csv("/Users/jack/Documents/GitHub/Data-Mining-Statistical-Learning/Final Project/sept22.csv")
```

## Abstract: summarize your question, your methods, your results, and your main conclusions in a few hundred words or less.



&nbsp;

## Introduction: Introduce the question you’re trying to answer at a reasonable level of detail. Give background and motivation for why it’s important.



&nbsp;

## Methods: Describe your data set and the methods you will use to analyze it.



&nbsp;

## Results: Tables, figures, and text that illustrate your findings. Keep the focus on the numbers here. You will interpret your results in the next section.

```{r 1}
####Data Cleaning
NY_Spring <- NY_Spring %>%
  mutate(spring = 1) %>%
  mutate(summer = 0) %>%
  mutate(fall = 0) %>%
  mutate(winter = 0)

#change the format
NY_Spring$host_since <- as.Date(NY_Spring$host_since, format = "%Y-%m-%d")
Data_collection_d <- as.Date("2023-03-15")
#since how many years ago he/she has been a host:
NY_Spring$host_since <- as.numeric(difftime(Data_collection_d, NY_Spring$host_since, units = "days")) / 365.25

NY_Summer <- NY_Summer %>%
  mutate(spring = 0) %>%
  mutate(summer = 1) %>%
  mutate(fall = 0) %>%
  mutate(winter = 0)

#change the format
NY_Summer$host_since <- as.Date(NY_Summer$host_since, format = "%Y-%m-%d")
Data_collection_d <- as.Date("2022-06-15")
#since how many years ago he/she has been a host:
NY_Summer$host_since <- as.numeric(difftime(Data_collection_d, NY_Summer$host_since, units = "days")) / 365.25

NY_Fall <- NY_Fall %>%
  mutate(spring = 0) %>%
  mutate(summer = 0) %>%
  mutate(fall = 1) %>%
  mutate(winter = 0)

#change the format
NY_Fall$host_since <- as.Date(NY_Fall$host_since, format = "%Y-%m-%d")
Data_collection_d <- as.Date("2022-09-15")
#since how many years ago he/she has been a host:
NY_Fall$host_since <- as.numeric(difftime(Data_collection_d, NY_Fall$host_since, units = "days")) / 365.25

NY_Winter <- NY_Winter %>%
  mutate(spring = 0) %>%
  mutate(summer = 0) %>%
  mutate(fall = 0) %>%
  mutate(winter = 1)

#change the format
NY_Winter$host_since <- as.Date(NY_Winter$host_since, format = "%Y-%m-%d")
Data_collection_d <- as.Date("2022-12-15")
#since how many years ago he/she has been a host:
NY_Winter$host_since <- as.numeric(difftime(Data_collection_d, NY_Winter$host_since, units = "days")) / 365.25

#merging seasons
NY_BNB <- rbind(NY_Spring, NY_Summer, NY_Fall, NY_Winter)

write.csv(NY_BNB, file = "/Users/jack/Documents/GitHub/Data-Mining-Statistical-Learning/Final Project/merged_data.csv", row.names = FALSE)

#remove unrelated columns
NY_BNB <- NY_BNB[, -c(2,3,4,5,8,9,10,11,12,14,15,16,17,18,19,20,21,22,23,25,26,28,33,36,43,44,45,46,47,48,49,50,51,56,58,59,60,61,69,71,72,73,74)]

#NY_BNB2 that contains only the rows of the original data frame NY_BNB
#that do not have any missing values.
NY_BNB2 <- NY_BNB[complete.cases(NY_BNB), ]

#change $price to integer
NY_BNB2$price <- as.integer(gsub("[,$]", "", NY_BNB2$price))

#change "f,t" format to 0 and 1
NY_BNB2$host_identity_verified <- ifelse(NY_BNB2$host_identity_verified == "t", 1, 0)
NY_BNB2$instant_bookable <- ifelse(NY_BNB2$instant_bookable == "t", 1, 0)

#drop if host_since <=1
NY_BNB2 = NY_BNB2 %>%
  filter(host_since >= 1)

# Creating dummies:
NY_BNB2 = NY_BNB2 %>%
  mutate(shared_room = ifelse(room_type == "Shared room", 1, 0))
NY_BNB2 = NY_BNB2 %>%
  mutate(private_room = ifelse(room_type == "Private room", 1, 0))
NY_BNB2 = NY_BNB2 %>%
  mutate (entire_home = ifelse(room_type == "Entire home/apt", 1, 0))
NY_BNB2 = NY_BNB2 %>%
  mutate (hotel_room = ifelse(room_type == "Hotel room", 1, 0))

# Random sample:
set.seed(1)
NY_reduced <- NY_BNB2 %>%
  sample_frac(0.25)

view(NY_reduced)
####the end of Data Cleaning
```


```{r 2}
## Understanding the Data
# Room Type
ggplot(NY_reduced, aes(x = room_type)) +
  geom_bar() +
  labs(x = "Room Type", y = "Frequency") 

# Neighborhood
ggplot(NY_reduced, aes(x = neighbourhood_group_cleansed)) +
  geom_bar() +
  labs(x = "Neighborhood", y = "Frequency") 

# Review Scores
ggplot(NY_reduced, aes(x = review_scores_rating))+
  geom_histogram() +
  labs(x = "Review Scores", y = "Frequency") 
```


```{r 3}
## Understanding Price

# Filter the data to the price range between 0 and 1000 USD
filtered_data <- NY_reduced[NY_reduced$price >= 0 & NY_reduced$price <= 1000, ]

# How many data points did we remove for the filtered price dataset?
total_observations <- nrow(NY_reduced)
filtered_observations <- nrow(filtered_data)
percentage_filtered <- (total_observations - filtered_observations) / total_observations * 100
cat("Percentage of filtered observations:", percentage_filtered, "%\n")

# We only lost ~0.9% of observations.

# Calculate the mean price per neighborhood
neighborhood_price <- filtered_data %>%
  group_by(neighbourhood_cleansed) %>%
  summarize(neighborhood_price = mean(price, na.rm = TRUE))

filtered_data <- merge(filtered_data, neighborhood_price, by = "neighbourhood_cleansed")

# Create a histogram using ggplot2
ggplot(filtered_data, aes(x = price)) +
  geom_histogram(binwidth = 10, color = "black", fill = "blue") +
  labs(title = "Histogram of Price Values (0 - 1000 USD)", x = "Price", y = "Frequency")

# Boxplot by neighborhood:
ggplot(filtered_data, aes(x = neighbourhood_group_cleansed, y = price)) +
  geom_boxplot(color = "black", fill = "blue") +
  labs(title = "Boxplot of Price by Neighborhood Group", x = "Neighborhood Group", y = "Price")

# Price by neighborhood
median_prices_neighborhood <- filtered_data %>%
  group_by(neighbourhood_group_cleansed) %>%
  summarize(median_price = median(price))

print(median_prices_neighborhood)

# Price by room type
median_prices_room_type <- NY_reduced %>%
  group_by(room_type) %>%
  summarize(median_price = median(price))

print(median_prices_room_type)

# Price by review score, price under 1000
ggplot(filtered_data, aes(x = price, y = review_scores_rating)) +
  geom_point() +
  labs(title = "Scatterplot of Price by Review Score", x = "Price", y = "Review Score")
```



```{r 4}
## Predicting Price ($0-1000)
set.seed(2)
filtered_data_split = initial_split(filtered_data, prop=0.8)
filtered_data_train = training(filtered_data_split)
filtered_data_test  = testing(filtered_data_split)

lm2 <- lm(log(price) ~ latitude*longitude + review_scores_rating + bedrooms + neighbourhood_group_cleansed + entire_home + shared_room + private_room, data = filtered_data_train)

#LASSO
# create your own numeric feature matrix.
x1 = sparse.model.matrix(log(price) ~ .-1, data=filtered_data) # do -1 to drop intercept!
y1 = filtered_data$price

# fit a single lasso
set.seed(2) # Set seed for reproducibility
lasso1 = gamlr(x1, y1, family="gaussian", penalty.factor=1)

# the coefficients at the AIC-optimizing value
scbeta1 = coef(lasso1)
scbeta1_nonzero <- which(scbeta1 != 0, arr.ind = TRUE)
# That's a lot of features...

# Linear Model, (some) LASSO features
lm_lasso1 <- lm(log(price) ~ latitude*longitude + host_since + neighbourhood_group_cleansed + host_total_listings_count + accommodates + bedrooms + minimum_nights + availability_30 + availability_90 + review_scores_cleanliness + review_scores_value + instant_bookable + entire_home + shared_room + private_room + spring + summer + fall, data = filtered_data_train)
modelr::rmse(lm_lasso1, filtered_data_test)

# Random Tree
# fit a single tree
BNB.tree1 = rpart(log(price) ~ host_since + neighbourhood_group_cleansed + host_total_listings_count + accommodates + bedrooms + minimum_nights + availability_30 + availability_90 + review_scores_cleanliness + review_scores_value + instant_bookable + entire_home + shared_room + private_room + spring + summer + fall + winter,
                       data=filtered_data_train, control = rpart.control(cp = 0.00001))
modelr::rmse(BNB.tree1, filtered_data_test)

# random forest
BNB.forest1 = randomForest(log(price) ~ latitude*longitude + host_since + neighbourhood_group_cleansed + host_total_listings_count + accommodates + bedrooms + minimum_nights + availability_30 + availability_90 + review_scores_cleanliness + review_scores_value + instant_bookable + entire_home + shared_room + private_room + spring + summer + fall + winter, data=filtered_data_train, importance=TRUE)
modelr::rmse(BNB.forest1, filtered_data_test)

# boosted model
BNB.boost1 = gbm(log(price) ~ latitude*longitude + host_since + host_total_listings_count + accommodates + bedrooms + minimum_nights + availability_30 + availability_90 + review_scores_cleanliness + review_scores_value + instant_bookable + entire_home + shared_room + private_room + spring + fall + winter, data=filtered_data_train, distribution = "gaussian", interaction.depth=6, n.trees=5000, shrinkage=.05, cv.folds = 2)
modelr::rmse(BNB.boost1, filtered_data_test)

# Root mean squared error - log price
RMSE3 = tibble("model" = c("lm2", "lm_lasso1", "BNB.tree1", "BNB.forest1", "BNB.boost1"), "RMSE" = c(rmse(lm2, filtered_data_test), rmse(lm_lasso1, filtered_data_test), rmse(BNB.tree1, filtered_data_test), rmse(BNB.forest1, filtered_data_test), rmse(BNB.boost1, filtered_data_test)))
RMSE3.1 = kable(RMSE3, caption = "RMSE of each model (price)")
RMSE3.1
```



```{r 5}
## PCA

# Extract the columns that are relevant to host 
host_relevant_columns <- NY_BNB2[, c("host_since", "host_total_listings_count", "host_identity_verified", "review_scores_accuracy", "review_scores_cleanliness", "review_scores_checkin", "review_scores_communication", "instant_bookable")]

host_correlation_matrix <- cor(host_relevant_columns)

ggcorrplot(host_correlation_matrix,
           hc.order = TRUE, # Reorder variables based on hierarchical clustering
           type = "full", # Show full correlation coefficients
           lab = TRUE, # Show correlation coefficient values on the plot
           lab_size = 4, # Set font size for correlation coefficient values
           tl.cex = 12, # Set font size for variable names
           colors = c("#3c7e9e", "white", "#d93a48"), # Set color palette for positive, neutral, and negative correlations
           title = "Correlation Plot for host_relevant_columns Matrix") # Set title for the plot


# Extract the columns that are relevant to place 
place_relevant_columns <- NY_BNB2[, c( "latitude", "longitude", "accommodates", "bedrooms", "beds", "shared_room", "private_room", "entire_home", "hotel_room", "review_scores_location")]
place_correlation_matrix <- cor(place_relevant_columns)
ggcorrplot(place_correlation_matrix,
           hc.order = TRUE, 
           type = "full",
           lab = TRUE, 
           lab_size = 4, 
           tl.cex = 12, 
           colors = c("#3c7e9e", "white", "#d93a48"), 
           title = "Correlation Plot for Relevant Columns") 

# Extract the columns that are relevant to performance 
performance_relevant_columns <- NY_BNB2[, c("number_of_reviews", "review_scores_rating", "review_scores_value", "reviews_per_month", "price")]
performance_correlation_matrix <- cor(performance_relevant_columns)
ggcorrplot(performance_correlation_matrix,
           hc.order = TRUE, 
           type = "full",
           lab = TRUE, 
           lab_size = 4, 
           tl.cex = 12, 
           colors = c("#3c7e9e", "white", "#d93a48"), 
           title = "Correlation Plot for Relevant Columns") 


# Now look at PCA  
PCA_1 = prcomp(host_relevant_columns, scale=TRUE, rank=2)

summary(PCA_1)

PCA_2 = prcomp(place_relevant_columns, scale=TRUE, rank=3)

summary(PCA_2)
# first few pcs
# try interpreting the loadings
# the question to ask is: "which variables does this load heavily on (positive and negatively)?"
round(PCA_1$rotation,2) 


alpha24=PCA_1$x
alpha25=PCA_2$x

df24 <- data.frame(alpha24)
df25 <- data.frame(alpha25)
df26 <- data.frame(performance_relevant_columns)



df26$Row.names <- row.names(df26)

station_data = merge(df24, df25, by = 'row.names')




# Add row.names as a separate column in each data frame
station_data$row.names <- row.names(station_data)

# Merge data frames by row.names
#merged_data = merge(
  
merged_data <- merge(station_data, df26, by = 'Row.names')


# Remove the redundant row.names column
merged_data$row.names <- NULL
merged_data$Row.names <- NULL



head(merged_data)

##&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&
set.seed(55)
merged_data_split =  initial_split(merged_data, prop=0.8)
merged_data_train = training(merged_data_split)
merged_data_test  = testing(merged_data_split)

# principal component regression: predicted engagement with 5 variables
lm_pca = lm(price ~ PC1.x + PC2.x + PC1.y + PC2.y + PC3 , data=merged_data_train)
summary(lm_pca)
modelr::rmse(lm_pca, merged_data_test)

plot(price ~ fitted(lm_pca), data=merged_data_train)
```



```{r 6}
# Price:
# get predicted values for all listings
forest1 = randomForest(log(price) ~ latitude*longitude + host_since + neighbourhood_group_cleansed + host_total_listings_count + accommodates + bedrooms + minimum_nights + availability_30 + availability_90 + review_scores_cleanliness + review_scores_value + instant_bookable + entire_home + shared_room + private_room + spring + summer + fall + winter, data=filtered_data, importance=TRUE)

#add predictions to data
filtered_data = filtered_data %>%
  mutate(price_forest1_pred = predict(forest1))

# Calculate the mean predicted log price per neighborhood
neighborhood_pred_price <- filtered_data %>%
  group_by(neighbourhood_cleansed) %>%
  summarize(neighborhood_predicted_price = mean(price_forest1_pred, na.rm = TRUE))

# Merge the data frames by the common column 'neighbourhood_cleansed'
filtered_data <- merge(filtered_data, neighborhood_pred_price, by = "neighbourhood_cleansed")

# Convert back from log
filtered_data = filtered_data %>%
  mutate(neighborhood_pred_price_conv = exp(neighborhood_predicted_price))

# create log price per neighborhood
neighborhood_log_price <- filtered_data %>%
  group_by(neighbourhood_cleansed) %>%
  summarize(neighborhood_log_price = mean(log(price), na.rm = TRUE))

filtered_data <- merge(filtered_data, neighborhood_log_price, by = "neighbourhood_cleansed")

#create error measurements
filtered_data <- filtered_data %>%
  mutate("resid1" = abs(neighborhood_predicted_price - neighborhood_log_price))%>%
  mutate("percentErr1" = resid1/neighborhood_price)
```



```{r 7}
## Geography of Price
library(plotly)

filtered_data_sf <- st_as_sf(filtered_data, coords = c("longitude", "latitude"), crs = 4326)

geojson_file <- "/Users/jack/Documents/GitHub/Data-Mining-Statistical-Learning/Final Project/Borough_Boundaries.geojson"

# Read the GeoJSON file
nyc_shapefile <- st_read(geojson_file)
nyc_shapefile2 <- nyc_shapefile %>%
  st_transform(crs = st_crs(filtered_data_sf))

## Mapping Price
# Predicted Price by Neighborhood
ggplot() +
  geom_sf(data = nyc_shapefile2, fill = "lightgray", color = "black", size = 0.2) +
  geom_sf(data = filtered_data_sf, aes(color = neighborhood_pred_price_conv), size = .5) +
  scale_color_gradientn(colours = viridisLite::viridis(10),
                        limits = c(0, 350),
                        name = "Predicted Price") +
  theme_minimal() +
  labs(title = "Predicted Price by Neighborhood",
       subtitle = "Color gradient represents predicted price",
       color = "Predicted Price")

# True Price by Neighborhood
ggplot() +
  geom_sf(data = nyc_shapefile2, fill = "lightgray", color = "black", size = 0.2) +
  geom_sf(data = filtered_data_sf, aes(color = neighborhood_price), size = .5) +
  scale_color_gradientn(colours = viridisLite::viridis(10),
                        limits = c(0, 350),
                        name = "True Price") +
  theme_minimal() +
  labs(title = "True Price by Neighborhood",
       subtitle = "Color gradient represents true price",
       color = "True Price")

# Residual Error of Price
ggplot() +
  geom_sf(data = nyc_shapefile2, fill = "lightgray", color = "black", size = 0.2) +
  geom_sf(data = filtered_data_sf, aes(color = 100*percentErr1), size = .5) +
  scale_color_gradientn(colours = viridisLite::plasma(10), name = "Residual Error Rate") +
  theme_minimal() +
  labs(title = "Residual Error Rate - Price",
       subtitle = "Color gradient represents residual error rates",
       color = "Residual Error Rate")
```



&nbsp;

## Conclusion: Interpret what you found. What are the main lessons we should take away from your report?



