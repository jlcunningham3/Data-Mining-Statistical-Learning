---
title: "Final Project"
author: "Jack Cunningham & Ali Fazl"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 
options(`mosaic:parallelMessage` = FALSE)
library(tidyverse)
library(gamlr)
library(tidytext)
library(dbscan)
library(parallel)
library(dplyr)
library(knitr)
library(glmnet)
library(tm)
library(ggplot2)
library(igraph)
library(arules)
library(arulesViz)
library(mosaic)
library(rpart)
library(rpart.plot)
library(rsample) 
library(randomForest)
library(lubridate)
library(caret)
library(Matrix)
library(modelr)
library(gbm)
library(pdp)
library(ggmap)
library(cluster)
library(tidycensus)
library(MazamaLocationUtils)
library(ggcorrplot)
library(tigris)
library(fuzzyjoin)
library(sf)
library(data.table)
num_cores <- detectCores()
NY_Spring <- read.csv("/Users/jack/Documents/GitHub/Data-Mining-Statistical-Learning/Final Project/listings 2.csv")
NY_Winter <- read.csv("/Users/jack/Documents/GitHub/Data-Mining-Statistical-Learning/Final Project/dec22.csv")
NY_Summer <- read.csv("/Users/jack/Documents/GitHub/Data-Mining-Statistical-Learning/Final Project/june22.csv")
NY_Fall <- read.csv("/Users/jack/Documents/GitHub/Data-Mining-Statistical-Learning/Final Project/sept22.csv")
```

```{r 1}
####Data Cleaning
NY_Spring <- NY_Spring %>%
  mutate(spring = 1) %>%
  mutate(summer = 0) %>%
  mutate(fall = 0) %>%
  mutate(winter = 0)

#change the format
NY_Spring$host_since <- as.Date(NY_Spring$host_since, format = "%Y-%m-%d")
Data_collection_d <- as.Date("2023-03-15")
#since how many years ago he/she has been a host:
NY_Spring$host_since <- as.numeric(difftime(Data_collection_d, NY_Spring$host_since,
                                            units = "days")) / 365.25

NY_Summer <- NY_Summer %>%
  mutate(spring = 0) %>%
  mutate(summer = 1) %>%
  mutate(fall = 0) %>%
  mutate(winter = 0)

#change the format
NY_Summer$host_since <- as.Date(NY_Summer$host_since, format = "%m/%d/%y")
Data_collection_d <- as.Date("2022-06-15")
#since how many years ago he/she has been a host:
NY_Summer$host_since <- as.numeric(difftime(Data_collection_d, NY_Summer$host_since,
                                            units = "days")) / 365.25

NY_Fall <- NY_Fall %>%
  mutate(spring = 0) %>%
  mutate(summer = 0) %>%
  mutate(fall = 1) %>%
  mutate(winter = 0)

#change the format
NY_Fall$host_since <- as.Date(NY_Fall$host_since, format = "%Y-%m-%d")
Data_collection_d <- as.Date("2022-09-15")
#since how many years ago he/she has been a host:
NY_Fall$host_since <- as.numeric(difftime(Data_collection_d, NY_Fall$host_since,
                                          units = "days")) / 365.25

NY_Winter <- NY_Winter %>%
  mutate(spring = 0) %>%
  mutate(summer = 0) %>%
  mutate(fall = 0) %>%
  mutate(winter = 1)

#change the format
NY_Winter$host_since <- as.Date(NY_Winter$host_since, format = "%Y-%m-%d")
Data_collection_d <- as.Date("2022-12-15")
#since how many years ago he/she has been a host:
NY_Winter$host_since <- as.numeric(difftime(Data_collection_d, NY_Winter$host_since,
                                            units = "days")) / 365.25

#merging seasons
NY_BNB <- rbind(NY_Spring, NY_Summer, NY_Fall, NY_Winter)

write.csv(NY_BNB, file = "/Users/jack/Documents/GitHub/Data-Mining-Statistical-Learning/Final Project/merged_data.csv", row.names = FALSE)

#remove unrelated columns
NY_BNB <- NY_BNB[, -c(2,3,4,5,8,9,10,11,12,14,15,16,17,18,19,20,21,22,23,
                      25,26,28,33,36,43,44,45,46,47,48,49,50,51,56,58,59,
                      60,61,69,71,72,73,74)]

#NY_BNB2 that contains only the rows of the original data frame NY_BNB
#that do not have any missing values.
NY_BNB2 <- NY_BNB[complete.cases(NY_BNB), ]

#change $price to integer
NY_BNB2$price <- as.integer(gsub("[,$]", "", NY_BNB2$price))
NY_BNB2 = NY_BNB2 %>%
  mutate(log_price = log(price))

#change "f,t" format to 0 and 1
NY_BNB2$host_identity_verified <- ifelse(NY_BNB2$host_identity_verified == "t", 1, 0)
NY_BNB2$instant_bookable <- ifelse(NY_BNB2$instant_bookable == "t", 1, 0)

#drop if host_since <=1
NY_BNB2 = NY_BNB2 %>%
  filter(host_since >= 1)

# Creating dummies:
NY_BNB2 = NY_BNB2 %>%
  mutate(shared_room = ifelse(room_type == "Shared room", 1, 0))
NY_BNB2 = NY_BNB2 %>%
  mutate(private_room = ifelse(room_type == "Private room", 1, 0))
NY_BNB2 = NY_BNB2 %>%
  mutate (entire_home = ifelse(room_type == "Entire home/apt", 1, 0))
NY_BNB2 = NY_BNB2 %>%
  mutate (hotel_room = ifelse(room_type == "Hotel room", 1, 0))

## Location clusters
coords <- cbind(NY_BNB2$latitude, NY_BNB2$longitude)

# Run the DBSCAN clustering algorithm
# Set the eps parameter (maximum distance between points in the same cluster)
# and the MinPts parameter (minimum number of points to form a dense region)
dbscan_result <- dbscan(coords, eps = 0.01, minPts = 5)

# Add the cluster assignments to the dataset
NY_BNB2$spatial_cluster <- as.factor(dbscan_result$cluster)

# Random sample:
set.seed(1)
NY_reduced <- NY_BNB2 %>%
  sample_frac(0.25)

view(NY_reduced)
####the end of Data Cleaning
```

## Abstract:

*summarize your question, your methods, your results, and your main conclusions in a few hundred words or less.*

 

## Introduction:

*Introduce the question you're trying to answer at a reasonable level of detail. Give background and motivation for why it's important.*

Our question is: What factors best predict AirBnB prices in New York City, and how can hosts and travelers use that information?

The rise of the sharing economy has transformed the way people travel and seek accommodations. Platforms such as AirBnB have gained popularity by allowing property owners to rent out their homes or rooms to travelers, offering an alternative to traditional hotels. New York City sees over 50 million visitors per year; the city has experienced significant growth in its AirBnB market, with tens of thousands of active listings at any given time. Accurate prediction of AirBnB prices is essential for hosts to optimize their revenue and for travelers to make informed decisions when selecting accommodations. This project aims to examine the factors connected to AirBnB prices in New York City, focusing on the role of geography.

The audience for this project is hosts and travelers who want to understand the factors that predict New York's AirBnB prices. Travelers might be interested in saving money; understanding the factors that are most important for price, and the spatial locations of the most and least expensive AirBnBs, could help in that goal. And hosts, seeking to maximize revenue, will be curious about the factors that predict price for the same reason travelers are. The model in this project gives insight into pricing for a wide range of properties across the city; hosts could use the predicted prices of similar properties to set their own pricing strategies.

Spatial geography turns out to be a key predictor of New York's AirBnB prices; as the saying goes, location, location, location. To better understand how well our predictive model performs in each neighborhood, we map the predicted prices of New York's AirBnBs against their actual prices, and we map the percent error of our model. This helps the reader understand where in the city our model does a good job predicting prices, and thus how well calibrated our model is to their neighborhood of interest.

In this project, we employ machine learning techniques, including LASSO regression, random forest, and gradient boosting, to best predict AirBnB prices based on a comprehensive set of variables. We leverage clustering algorithms, ie. DBSCAN, to capture spatial patterns in the data and map predicted vs. actual prices. By doing so, we aim to enhance our understanding of the factors driving AirBnB prices in New York City and provide valuable insights for hosts and travelers alike.

 

## Methods:

### Data

For this project, we use four datasets combined into one. Each dataset contains the entire set of scraped NYC AirBnB listings at the following dates: June 15, 2022; September 15, 2022; December 15, 2022; and March 15, 2023. The data contain 70+ variables and more than 160,000 total listings (roughly 40,000 per quarter). These data come from InsideAirbnb: <http://insideairbnb.com/get-the-data/>.

We made some important modifications to the dataset in order to meet our needs:

-   Creating dummy variables for each season (June is summer, September is fall, December is winter, and March is spring), depending on which initial dataset the observation came from.

-   Modifying the host_since variable to provide a number of years since the start of the host's presence on AirBnB.

-   Removing roughly half the columns which we did not use in our analysis.

-   Dropping all observations with host_since \< 1 year. This ensures that all the listings come from hosts who have been on the platform for at least one year, helping to ease concerns about seasonal effects.

-   Dropping all observations with NA values in any of the fields. Among other effects, this ensures that our dataset includes only listings with at least one review, host-provided descriptions, and complete information about amenities, bedrooms, etc.

-   Manipulating variables to be easier to work with, e.g. adding dummy variables for room_type and changing f/t format to 0/1.

* Using the DBSCAN clustering algorithm to spatially cluster the listings, to create another spatial variable beyond latitude and longitude to use as a predictor in our models.

-   Filtering price outliers (those over \$1500, roughly 0.4% of our dataset).

We then took a random sample of 25% of the cleaned data to use for our analysis. This sample still has over 25,000 observations of 42 variables. Reducing the dataset via random selection makes it easier to work with computationally while not sacrificing much accuracy, especially since we are working with full data on all of NYC's AirBnBs.

The most important reason we combined the four datasets into one is that it gives us a snapshot of seasonality. We only have price data for the dates that each dataset was scraped (6/15/22, 9/15/22, 12/15/22, and 3/15/23). One limitation of our dataset is that we don't have daily price data for a year; this would have been desired in order to uncover seasonal trends and variation with more fidelity. However, quarterly price data provides a rough proxy of seasonal price trends. As our results will show, season is not a particularly important predictor of NYC AirBnB prices.

One other important note about our dataset is that we are treating each listing as unique, despite the fact that many of the listings are run by the same hosts in each period. The reason for this approach is that many of the variables we use as predictors can change from one quarter to the next; e.g., review scores, availability data, and number of reviews, among many more. Hence, it makes more sense to treat each observation as unique, rather than simply extracting the seasonal prices for each listing and attaching them to one of the seasonal datasets.

The plots that follow give a sense of some of the important variables in this dataset, including frequency of listings of each room type and in each borough, and the distribution of review scores.

```{r 2}
## Understanding the Data
# Room Type
ggplot(NY_reduced, aes(x = room_type)) +
  geom_bar() +
  labs(x = "Room Type", y = "Frequency") 

# Neighborhood
ggplot(NY_reduced, aes(x = neighbourhood_group_cleansed)) +
  geom_bar() +
  labs(x = "Neighborhood", y = "Frequency") 

# Review Scores
ggplot(NY_reduced, aes(x = review_scores_rating))+
  geom_histogram(binwidth = 0.1) +
  labs(x = "Review Scores", y = "Frequency") 
```

Room type is split relatively evenly between entire home/apt and private room, with hotel room and shared room comprising very few of New York's AirBnB listings. The properties are concentrated in Manhattan and Brooklyn, each with more than double the number of listings Queens has; the Bronx and Staten Island have comparatively very few listings. And the average review score for each listing is heavily right-skewed, with very few listings at a sub-4 review score. More analysis of price follows in the Results section.

 

### Approach

We used a selection of methods in order to create the best price prediction model possible with this dataset. We trained each model on 80% of the cleaned dataset, then tested its root mean squared error using the remaining 20% of the dataset. The outcome of interest here is log price instead of price, in order to help normalize the distribution; as we will demonstrate below, the price variable is heavily left-skewed. This transformation also helps in the case that the relationship between our predictors and the price is non-linear.

We first tried a simple linear model, including just a few factors we thought would likely prove important, including location, property type, and review score. We then used the LASSO method with all the variables in the cleaned dataset in order to determine the most important ones; then, we ran a linear model using only the variables selected in the LASSO process.

Then, we turned to more sophisticated machine learning techniques: random tree, random forest, and gradient boosting. We tweaked each of these models via trial-and-error, adding and removing predictor variables as appropriate for the model. The random forest model turned out to have the best performance; we plotted the importance of each variable in this model in order to determine the most important factors affecting price.

Finally, in order to map our selected model's performance in each neighborhood, we first added the predicted price values from the random forest model to the dataset for each listing. Then, we calculated the mean predicted price in each of the 235 distinct New York City neighborhoods in our dataset, along with the actual mean prices for each neighborhood, and calculated the error rate. Finally, we plotted all three of these measures for each listing by latitude and longitude, along with NYC's geographic boundaries.

As an aside, we'd like to mention a couple of approaches to this question we tried that did not work, or did not prove useful, and are thus not included below. They include:

* Completing all the same analysis, but for occupancy rate: Our original question was about predicting New York's AirBnB occupancy rate, in addition to price. However, occupancy rate was not a variable we had access to in the dataset, so we sought to construct the occupancy rate for each listing. We realized that the algorithm that others who have studied AirBnB's occupancy rates have used (monthly reviews x expected review rate x average stay length) relied on the faulty assumptions that expected review rate and stay length are constant for all listings in the dataset. Rather than using an algorithm based on such faulty assumptions, we decided to forgo our analysis of occupancy rate and instead limit our analysis to price.

* Text analysis: We tried using the topic modeling technique of Latent Dirichlet Allocation (LDA) to extract key words and topics from the names and descriptions of the listings. However, upon performing this analysis, the topics were insufficiently differentiated from each other (e.g., four topics under the heading "apartment" in the top 10), and the coefficient of each topic was so low, that they yielded very little insight into predicted price.



 

## Results:

### Preliminary Analysis of Price:

Price in our dataset is left-skewed; while some listings exceed \$1000 per night, 75% of listings are \$200 or less. 

```{r 3}
## Understanding Price

# Filter the data to the price range between 0 and 1500 USD
filtered_data <- NY_reduced[NY_reduced$price >= 0 & NY_reduced$price <= 1500, ]

# How many data points did we remove for the filtered price dataset?
total_observations <- nrow(NY_reduced)
filtered_observations <- nrow(filtered_data)
percentage_filtered <- (total_observations - filtered_observations) / total_observations * 100

# We only lost ~0.4% of observations.

# Calculate the mean price per neighborhood
neighborhood_price <- filtered_data %>%
  group_by(neighbourhood_cleansed) %>%
  summarize(neighborhood_price = mean(price, na.rm = TRUE))

filtered_data <- merge(filtered_data, neighborhood_price, by = "neighbourhood_cleansed")

# Create a histogram using ggplot2
ggplot(filtered_data, aes(x = price)) +
  geom_histogram(binwidth = 10, color = "black", fill = "blue") +
  labs(title = "Histogram of Price Values (0 - 1500 USD)", x = "Price", y = "Frequency")
```

Manhattan has the most expensive listings, with a median nightly price of \$160. Brooklyn follows at \$119 per night; listings in the other boroughs have median nightly prices between \$80 and \$100. Each borough has quite a few upper outliers in price, as demonstrated by the boxplot below.

```{r 3.1}
# Boxplot by neighborhood:
ggplot(filtered_data, aes(x = neighbourhood_group_cleansed, y = price)) +
  geom_boxplot(color = "black", fill = "blue") +
  labs(title = "Boxplot of Price by Neighborhood Group", 
       x = "Neighborhood Group", y = "Price")

# Price by neighborhood
median_prices_neighborhood <- filtered_data %>%
  group_by(neighbourhood_group_cleansed) %>%
  summarize(median_price = median(price))

kable(median_prices_neighborhood)
```

Below, we've included the median listing price across the city by room type and season. Hotel rooms are by far the most expensive, although they are the least common. Entire homes are more than twice as expensive as private rooms, and nearly three times as expensive as shared rooms.

Season appears not to affect citywide median price much. The median price in each season ranges from \$120 in spring to \$130 in winter. Again, since we only have price data for one date per season, we lack the fine-grained price data that would be useful to identify bigger seasonal shifts, or price trends for each neighborhood (especially during e.g. big events or holidays). But for many listings in our dataset, the listed price was identical for each of the four dates of scraped data. It's possible that many AirBnB hosts don't change their prices much or at all from the default, although without better data this hypothesis is just speculation.

At the borough level, there is a bit more seasonal variation in median price in some boroughs and less in others. The range of Staten Island's median price by season is 25, while the range of Queens's median price by season is just 4.5.

```{r 3.2}
# Price by room type
median_prices_room_type <- NY_reduced %>%
  group_by(room_type) %>%
  summarize(median_price = median(price))

kable(median_prices_room_type)

# Price by season
# Compute the mean price for each season
median_price_by_season <- NY_reduced %>%
  gather(season, flag, spring:winter) %>%
  filter(flag == 1) %>%
  group_by(season) %>%
  summarise(median_price = median(price, na.rm = TRUE))

kable(median_price_by_season)

# Calculate median price by neighborhood per season
median_price_by_neighborhood_season <- NY_reduced %>%
  gather(season, flag, spring:winter) %>%
  filter(flag == 1) %>%
  group_by(neighbourhood_group_cleansed, season) %>%
  summarise(median_price = median(price, na.rm = TRUE))

# Print the result
kable(median_price_by_neighborhood_season)
```

 

### Building a Prediction Model

The table below gives the RMSE of each model we tested. The best-performing model is the random forest model, which has a RMSE of 0.33; on average, this model is roughly 33% off of the true price.

```{r 4}
## Predicting Price ($0-1500)
set.seed(2)
filtered_data_split = initial_split(filtered_data, prop=0.8)
filtered_data_train = training(filtered_data_split)
filtered_data_test  = testing(filtered_data_split)

lm2 <- lm(log_price ~ latitude*longitude + review_scores_rating + bedrooms +
            neighbourhood_group_cleansed + entire_home + shared_room + private_room, 
          data = filtered_data_train)

#LASSO
# create your own numeric feature matrix.
x1 = sparse.model.matrix(log_price ~ .-1 - price - name - description - 
                           amenities - neighbourhood_cleansed, 
                         data=filtered_data)
y1 = filtered_data$price

# fit a single lasso
set.seed(2) # Set seed for reproducibility
lasso1 = gamlr(x1, y1, family="gaussian", penalty.factor=1)

# the coefficients at the AIC-optimizing value
scbeta1 = coef(lasso1)
scbeta1_nonzero <- which(scbeta1 != 0, arr.ind = TRUE)

# Linear Model, (some) LASSO features
lm_lasso1 <- lm(log_price ~ neighbourhood_group_cleansed +
                  host_total_listings_count + bathrooms_text +
                  number_of_reviews + entire_home + shared_room +
                  reviews_per_month + review_scores_location + review_scores_value +
                  spring + fall, data = filtered_data_train)

# random tree
BNB.tree1 = rpart(log_price ~ spatial_cluster + host_since + 
                    neighbourhood_group_cleansed + host_total_listings_count +
                    accommodates + bedrooms + minimum_nights + availability_30 +
                    availability_90 + number_of_reviews + review_scores_cleanliness +
                    reviews_per_month + review_scores_location + review_scores_value +
                    review_scores_rating + instant_bookable + entire_home + 
                    shared_room + private_room + spring + summer + fall + winter,
                       data=filtered_data_train, control = rpart.control(cp = 0.00001))

# random forest
BNB.forest1 = randomForest(log_price ~ spatial_cluster + latitude*longitude + 
                             host_since + neighbourhood_group_cleansed +
                             host_total_listings_count + accommodates + bedrooms +
                             minimum_nights + availability_30 + availability_90 +
                             reviews_per_month + review_scores_location +
                             number_of_reviews + review_scores_cleanliness +
                             review_scores_value + review_scores_rating +
                             instant_bookable + entire_home + shared_room + 
                             private_room + spring + summer + fall + winter,
                           data=filtered_data_train, importance=TRUE)

# boosted model
BNB.boost1 = gbm(log_price ~ spatial_cluster + latitude*longitude + host_since +
                   host_total_listings_count + accommodates + bedrooms + 
                   minimum_nights + availability_30 + availability_90 +
                   review_scores_cleanliness + number_of_reviews + reviews_per_month +
                   review_scores_location + review_scores_value + review_scores_rating +
                   instant_bookable + entire_home + shared_room + private_room + 
                   spring + summer + fall + winter, 
                 data=filtered_data_train, distribution = "gaussian",
                 interaction.depth=6, n.trees=5000, shrinkage=.05, cv.folds = 2)

# Root mean squared error - log price
RMSE3 = tibble("model" = c("lm2", "lm_lasso1", "BNB.tree1", "BNB.forest1", "BNB.boost1"),
               "RMSE" = c(rmse(lm2, filtered_data_test), 
                          rmse(lm_lasso1, filtered_data_test), 
                          rmse(BNB.tree1, filtered_data_test), 
                          rmse(BNB.forest1, filtered_data_test), 
                          rmse(BNB.boost1, filtered_data_test)))
RMSE3.1 = kable(RMSE3, caption = "RMSE of each model (log price)")
RMSE3.1
```

Below, we see the variable importance plot. In general, the variables that have the most impact on the model are latitude & longitude (location), room type, minimum length of stay, number of guests the listing can accommodate, and reviews per month. The least important variables in our model are, strangely, the seasonal variables and the spatial clusters created by the DBSCAN clustering algorithm.

```{r 4.1}
# Variable Importance:
varImpPlot(BNB.forest1)
```

 

### Mapping

Below, we see first the map of mean predicted price by neighborhood (converted back from log form for interpretability), followed by the map of actual mean price by neighborhood (not borough; recall that, while there are 5 boroughs in NYC, there are over 250 neighborhoods in our dataset). 

```{r 6}
# Additional Data Manipulation
# get predicted values for all listings
forest1 = randomForest(log_price ~ spatial_cluster + latitude*longitude + host_since +
                         neighbourhood_group_cleansed + host_total_listings_count +
                         accommodates + bedrooms + minimum_nights + availability_30 +
                         availability_90 + reviews_per_month + review_scores_location +
                         number_of_reviews + review_scores_cleanliness +
                         review_scores_value + review_scores_rating + instant_bookable +
                         entire_home + shared_room + private_room + spring + summer +
                         fall + winter, data=filtered_data, importance=TRUE)

#add predictions to data
filtered_data = filtered_data %>%
  mutate(price_forest1_pred = predict(forest1))

# Calculate the mean predicted log price per neighborhood
neighborhood_pred_price <- filtered_data %>%
  group_by(neighbourhood_cleansed) %>%
  summarize(neighborhood_predicted_price = mean(price_forest1_pred, na.rm = TRUE))

# Merge the data frames by the common column 'neighbourhood_cleansed'
filtered_data <- merge(filtered_data, neighborhood_pred_price, 
                       by = "neighbourhood_cleansed")

# Convert back from log
filtered_data = filtered_data %>%
  mutate(neighborhood_pred_price_conv = exp(neighborhood_predicted_price))

# create log price per neighborhood
neighborhood_log_price <- filtered_data %>%
  group_by(neighbourhood_cleansed) %>%
  summarize(neighborhood_log_price = mean(log_price, na.rm = TRUE))

filtered_data <- merge(filtered_data, neighborhood_log_price, 
                       by = "neighbourhood_cleansed")

#create error measurements
filtered_data <- filtered_data %>%
  mutate("resid1" = abs(neighborhood_predicted_price - neighborhood_log_price))%>%
  mutate("percentErr1" = resid1/neighborhood_log_price)
```

```{r 7}
## Geography of Price
library(plotly)

filtered_data_sf <- st_as_sf(filtered_data, coords = c("longitude", "latitude"), 
                             crs = 4326)

geojson_file <- "/Users/jack/Documents/GitHub/Data-Mining-Statistical-Learning/Final Project/Borough_Boundaries.geojson"

# Read the GeoJSON file
nyc_shapefile <- st_read(geojson_file, quiet=TRUE)
nyc_shapefile2 <- nyc_shapefile %>%
  st_transform(crs = st_crs(filtered_data_sf))

## Mapping Price
# Predicted Price by Neighborhood
ggplot() +
  geom_sf(data = nyc_shapefile2, fill = "lightgray", color = "black", size = 0.2) +
  geom_sf(data = filtered_data_sf, aes(color = neighborhood_pred_price_conv), size = .5) +
  scale_color_gradientn(colours = viridisLite::viridis(10),
                        limits = c(0, 400),
                        name = "Predicted Price") +
  theme_minimal() +
  labs(title = "Predicted Price by Neighborhood",
       subtitle = "Color gradient represents predicted price",
       color = "Predicted Price")
```

```{r 7.1}
# True Price by Neighborhood
ggplot() +
  geom_sf(data = nyc_shapefile2, fill = "lightgray", color = "black", size = 0.2) +
  geom_sf(data = filtered_data_sf, aes(color = neighborhood_price), size = .5) +
  scale_color_gradientn(colours = viridisLite::viridis(10),
                        limits = c(0, 400),
                        name = "True Price") +
  theme_minimal() +
  labs(title = "True Price by Neighborhood",
       subtitle = "Color gradient represents true price",
       color = "True Price")
```

&nbsp;

And below, we see the mean residual error rate of our model's predicted price in each neighborhood:

```{r 7.2}
# Residual Error of Price
ggplot() +
  geom_sf(data = nyc_shapefile2, fill = "lightgray", color = "black", size = 0.2) +
  geom_sf(data = filtered_data_sf, aes(color = 100*percentErr1), size = .5) +
  scale_color_gradientn(colours = viridisLite::plasma(10),
                        name = "Residual Error Rate") +
  theme_minimal() +
  labs(title = "Residual Error Rate by Neighborhood - Log Price",
       subtitle = "Color gradient represents residual error rate",
       color = "Residual Error Rate")
```

 

## Conclusion:
*Interpret what you found. What are the main lessons we should take away from your report?*


