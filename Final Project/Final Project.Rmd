---
title: "Final Project"
author: "Jack Cunningham & Ali Fazl"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 
options(`mosaic:parallelMessage` = FALSE)
library(cowplot)
library(tidyverse)
library(gamlr)
library(gridExtra)
library(tidytext)
library(dbscan)
library(parallel)
library(dplyr)
library(knitr)
library(glmnet)
library(tm)
library(ggplot2)
library(igraph)
library(arules)
library(arulesViz)
library(mosaic)
library(rpart)
library(rpart.plot)
library(rsample) 
library(randomForest)
library(lubridate)
library(caret)
library(Matrix)
library(modelr)
library(gbm)
library(pdp)
library(ggmap)
library(cluster)
library(tidycensus)
library(MazamaLocationUtils)
library(ggcorrplot)
library(tigris)
library(fuzzyjoin)
library(sf)
library(data.table)
num_cores <- detectCores()
NY_Spring <- read.csv("/Users/jack/Documents/GitHub/Data-Mining-Statistical-Learning/Final Project/listings 2.csv")
NY_Winter <- read.csv("/Users/jack/Documents/GitHub/Data-Mining-Statistical-Learning/Final Project/dec22.csv")
NY_Summer <- read.csv("/Users/jack/Documents/GitHub/Data-Mining-Statistical-Learning/Final Project/june22.csv")
NY_Fall <- read.csv("/Users/jack/Documents/GitHub/Data-Mining-Statistical-Learning/Final Project/sept22.csv")
```

```{r 1}
####Data Cleaning
NY_Spring <- NY_Spring %>%
  mutate(spring = 1) %>%
  mutate(summer = 0) %>%
  mutate(fall = 0) %>%
  mutate(winter = 0)

#change the format
NY_Spring$host_since <- as.Date(NY_Spring$host_since, format = "%Y-%m-%d")
Data_collection_d <- as.Date("2023-03-15")
#since how many years ago he/she has been a host:
NY_Spring$host_since <- as.numeric(difftime(Data_collection_d, NY_Spring$host_since,
                                            units = "days")) / 365.25

NY_Summer <- NY_Summer %>%
  mutate(spring = 0) %>%
  mutate(summer = 1) %>%
  mutate(fall = 0) %>%
  mutate(winter = 0)

#change the format
NY_Summer$host_since <- as.Date(NY_Summer$host_since, format = "%m/%d/%y")
Data_collection_d <- as.Date("2022-06-15")
#since how many years ago he/she has been a host:
NY_Summer$host_since <- as.numeric(difftime(Data_collection_d, NY_Summer$host_since,
                                            units = "days")) / 365.25

NY_Fall <- NY_Fall %>%
  mutate(spring = 0) %>%
  mutate(summer = 0) %>%
  mutate(fall = 1) %>%
  mutate(winter = 0)

#change the format
NY_Fall$host_since <- as.Date(NY_Fall$host_since, format = "%Y-%m-%d")
Data_collection_d <- as.Date("2022-09-15")
#since how many years ago he/she has been a host:
NY_Fall$host_since <- as.numeric(difftime(Data_collection_d, NY_Fall$host_since,
                                          units = "days")) / 365.25

NY_Winter <- NY_Winter %>%
  mutate(spring = 0) %>%
  mutate(summer = 0) %>%
  mutate(fall = 0) %>%
  mutate(winter = 1)

#change the format
NY_Winter$host_since <- as.Date(NY_Winter$host_since, format = "%Y-%m-%d")
Data_collection_d <- as.Date("2022-12-15")
#since how many years ago he/she has been a host:
NY_Winter$host_since <- as.numeric(difftime(Data_collection_d, NY_Winter$host_since,
                                            units = "days")) / 365.25

#merging seasons
NY_BNB <- rbind(NY_Spring, NY_Summer, NY_Fall, NY_Winter)

write.csv(NY_BNB, file = "/Users/jack/Documents/GitHub/Data-Mining-Statistical-Learning/Final Project/merged_data.csv",
          row.names = FALSE)

#remove unused columns
NY_BNB <- NY_BNB[, -c(2,3,4,5,8,9,10,11,12,14,15,16,17,18,19,20,21,22,23,
                      25,26,28,33,36,43,44,45,46,47,48,49,50,51,56,58,59,
                      60,61,69,71,72,73,74)]

#NY_BNB2 that contains only the rows of the original data frame NY_BNB
#that do not have any missing values.
NY_BNB2 <- NY_BNB[complete.cases(NY_BNB), ]

#change $price to integer
NY_BNB2$price <- as.integer(gsub("[,$]", "", NY_BNB2$price))
NY_BNB2 = NY_BNB2 %>%
  mutate(log_price = log(price))

#change "f,t" format to 0 and 1
NY_BNB2$host_identity_verified <- ifelse(NY_BNB2$host_identity_verified == "t", 1, 0)
NY_BNB2$instant_bookable <- ifelse(NY_BNB2$instant_bookable == "t", 1, 0)

#drop if host_since <=1
NY_BNB2 = NY_BNB2 %>%
  filter(host_since >= 1)

# Creating dummies:
NY_BNB2 = NY_BNB2 %>%
  mutate(shared_room = ifelse(room_type == "Shared room", 1, 0))
NY_BNB2 = NY_BNB2 %>%
  mutate(private_room = ifelse(room_type == "Private room", 1, 0))
NY_BNB2 = NY_BNB2 %>%
  mutate (entire_home = ifelse(room_type == "Entire home/apt", 1, 0))
NY_BNB2 = NY_BNB2 %>%
  mutate (hotel_room = ifelse(room_type == "Hotel room", 1, 0))

# amenities:
words_to_search <- c("Pets allowed", "Kitchen", "Elevator", "Air conditioning", 
                     "Heating", "Long term stays allowed")
NY_BNB2 <- NY_BNB2 %>%
  mutate(Pets_allowed = as.integer(grepl(words_to_search[1], amenities)),
         Kitchen = as.integer(grepl(words_to_search[2], amenities)),
         Elevator = as.integer(grepl(words_to_search[3], amenities)),
         Air_conditioning = as.integer(grepl(words_to_search[4], amenities)),
         Heating = as.integer(grepl(words_to_search[5], amenities)),
         Long_term_allowed = as.integer(grepl(words_to_search[6], amenities)))

# Random sample:
set.seed(1)
NY_reduced <- NY_BNB2 %>%
  sample_frac(0.25)

## Location clusters
coords <- cbind(NY_reduced$latitude, NY_reduced$longitude)

# Run the DBSCAN clustering algorithm
# Set the eps parameter (maximum distance between points in the same cluster)
# and the MinPts parameter (minimum number of points to form a dense region)
dbscan_result <- dbscan(coords, eps = 0.0025, minPts = 20)

# Add the cluster assignments to the dataset
NY_reduced$spatial_cluster <- as.factor(dbscan_result$cluster)

# Filter the data to the price range between 0 and 1500 USD
filtered_data <- NY_reduced[NY_reduced$price >= 0 & NY_reduced$price <= 1500, ]

# How many data points did we remove for the filtered price dataset?
total_observations <- nrow(NY_reduced)
filtered_observations <- nrow(filtered_data)
percentage_filtered <- (total_observations - filtered_observations) / total_observations * 100

# We only lost ~0.4% of observations.

view(filtered_data)
####the end of Data Cleaning
```

## Abstract:

*summarize your question, your methods, your results, and your main conclusions in a few hundred words or less.*

 

## Introduction:

*Introduce the question you're trying to answer at a reasonable level of detail. Give background and motivation for why it's important.*

Our question is: What factors best predict AirBnB prices in New York City, and how can hosts and travelers use that information?

The rise of the sharing economy has transformed the way people travel and seek accommodations. Platforms such as AirBnB have gained popularity by allowing property owners to rent out their homes or rooms to travelers, offering an alternative to traditional hotels. New York City sees over 50 million visitors per year; the city has experienced significant growth in its AirBnB market, with tens of thousands of active listings at any given time. Accurate prediction of AirBnB prices is essential for hosts to optimize their revenue and for travelers to make informed decisions when selecting accommodations. This project aims to examine the factors connected to AirBnB prices in New York City, focusing on the role of geography.

The audience for this project is hosts and travelers who want to understand the factors that predict New York's AirBnB prices. Travelers might be interested in saving money; understanding the factors that are most important for price, and the spatial locations of the most and least expensive AirBnBs, could help in that goal. And hosts, seeking to maximize revenue, will be curious about the factors that predict price for the same reason travelers are. The model in this project gives insight into pricing for a wide range of properties across the city; hosts could use the predicted prices of similar properties to set their own pricing strategies.

Spatial geography turns out to be a key predictor of New York's AirBnB prices; as the saying goes, location, location, location. To better understand how well our predictive model performs in each neighborhood, we map the predicted prices of New York's AirBnBs against their actual prices, and we map the percent error of our model. This helps the reader understand where in the city our model does a good job predicting prices, and thus how well calibrated our model is to their neighborhood of interest.

In this project, we employ machine learning techniques, including LASSO regression, random forest, and gradient boosting, to best predict AirBnB prices based on a comprehensive set of variables. We leverage clustering algorithms, ie. DBSCAN, to capture spatial patterns in the data and map predicted vs. actual prices. By doing so, we aim to enhance our understanding of the factors driving AirBnB prices in New York City and provide valuable insights for hosts and travelers alike.

 

## Methods:

### Data

For this project, we use four datasets combined into one. Each dataset contains the entire set of scraped NYC AirBnB listings at the following dates: June 15, 2022; September 15, 2022; December 15, 2022; and March 15, 2023. The data contain 70+ variables and more than 160,000 total listings (roughly 40,000 per quarter). These data come from InsideAirbnb: <http://insideairbnb.com/get-the-data/>.

We made some important modifications to the dataset in order to meet our needs:

-   Creating dummy variables for each season (June is summer, September is fall, December is winter, and March is spring), depending on which initial dataset the observation came from.

-   Modifying the host_since variable to provide a number of years since the start of the host's presence on AirBnB.

-   Removing roughly half the columns which we did not use in our analysis.

-   Dropping all observations with host_since \< 1 year. This ensures that all the listings come from hosts who have been on the platform for at least one year, helping to ease concerns about seasonal effects.

-   Dropping all observations with NA values in any of the fields. Among other effects, this ensures that our dataset includes only listings with at least one review, host-provided descriptions, and complete information about amenities, bedrooms, etc.

-   Manipulating variables to be easier to work with, e.g. adding dummy variables for room_type and changing f/t format to 0/1.

-   Using the DBSCAN clustering algorithm to spatially cluster the listings, to create another spatial variable beyond latitude and longitude to use as a predictor in our models.

-   Extracting six important terms from the "amenities" list to use as predictors in our models.

-   Filtering price outliers (those over \$1500, roughly 0.4% of our dataset).

We then took a random sample of 25% of the cleaned data to use for our analysis. This sample still has over 25,000 observations of 42 variables. Reducing the dataset via random selection makes it easier to work with computationally while not sacrificing much accuracy, especially since we are working with full data on all of NYC's AirBnBs.

The most important reason we combined the four datasets into one is that it gives us a snapshot of seasonality. We only have price data for the dates that each dataset was scraped (6/15/22, 9/15/22, 12/15/22, and 3/15/23). One limitation of our dataset is that we don't have daily price data for a year; this would have been desired in order to uncover seasonal trends and variation with more fidelity. However, quarterly price data provides a rough proxy of seasonal price trends. As our results will show, season is not a particularly important predictor of NYC AirBnB prices.

One other important note about our dataset is that we are treating each listing as unique, despite the fact that many of the listings are run by the same hosts in each period. The reason for this approach is that many of the variables we use as predictors can change from one quarter to the next; e.g., review scores, availability data, and number of reviews, among many more. Hence, it makes more sense to treat each observation as unique, rather than simply extracting the seasonal prices for each listing and attaching them to one of the seasonal datasets.

The plots that follow give a sense of how price relates to some of the important variables in this dataset, including how many guests the listing can accommodate, minimum length of stay, listings of each room type, and overall review scores.

&nbsp;

```{r 2, fig.height=3.5}
## Understanding the Data
# Calculate mean price for each integer value of accommodates
mean_price_by_accommodates <- filtered_data %>%
  group_by(accommodates) %>%
  summarize(mean_price = mean(price, na.rm = TRUE))

# Plot the mean price for each integer value of accommodates
plot_1 <- ggplot(mean_price_by_accommodates, aes(x = accommodates, y = mean_price)) +
  geom_point() +
  geom_line(group = 1) +
  labs(title = "Mean Price by Accommodates",
       x = "Accommodates",
       y = "Mean Price") +
  theme_minimal()

# Calculate mean price for each integer value of minimum stay
mean_price_by_minimum_nights <- filtered_data %>%
  group_by(minimum_nights) %>%
  summarize(mean_price = mean(price, na.rm = TRUE))

# Plot the mean price for each integer value of accommodates
plot_2 <- ggplot(mean_price_by_minimum_nights, aes(x = minimum_nights, y = mean_price)) +
  geom_point() +
  geom_line(group = 1) +
  labs(title = "Mean Price by Minimum Nights",
       x = "Minimum Nights",
       y = "Mean Price") +
  theme_minimal() + 
  scale_x_continuous(limits = c(0, 35)) 

# Price - Room Type
plot_3 <- ggplot(filtered_data, aes(x = room_type, y = price)) +
  geom_boxplot(fill = "green") +
  labs(x = "Room Type", y = "Price") 

# Price - Review Scores
plot_4 <- ggplot(filtered_data, aes(x = review_scores_rating, y = price))+
  geom_point(size = 0.1) +
  labs(x = "Overall Review Score", y = "Price") 

plot_1
```

&nbsp;

```{r 2.1, fig.height=3.5}
plot_2
```

&nbsp;

```{r 2.2, fig.height=3.5}
plot_3
```

&nbsp;

```{r 2.3, fig.height=3.5}
plot_4
```

&nbsp;

These plots demonstrate that some of the variables that we think have a big impact on price, like accommodates and room type, have a clear (if noisy) relationship, while others are much less clear. For example, it's hard to tell if there's a downward relationship between minimum price and nights or if that's just noise in the dataset. And in the review score plot, there is so much mass in the square between 4 and 5 review score and \$0 and \$500 price that it's hard to see any relationship, other than with outliers.

Given this initial glance at the dataset, it's clear that we will have our work cut out for us in trying to predict price. In the next section, we will describe the approach we took in order to build the best predictive model of price possible, despite the noise.

 

### Approach

We used a selection of methods in order to create the best price prediction model possible with this dataset. We trained each model on 80% of the cleaned dataset, then tested its root mean squared error using the remaining 20% of the dataset. The outcome of interest here is log price instead of price, in order to help normalize the distribution; as we will demonstrate below, the price variable is heavily left-skewed. This transformation also helps in the case that the relationship between our predictors and the price is non-linear.

We first tried a simple linear model, including just a few factors we thought would likely prove important, including location, property type, and review score. We then used the LASSO method with all the variables in the cleaned dataset in order to determine the most important ones; then, we ran a linear model using only the variables selected in the LASSO process.

Then, we turned to more sophisticated machine learning techniques: random tree, random forest, and gradient boosting. We tweaked each of these models via trial-and-error, adding and removing predictor variables as appropriate for the model. The random forest model turned out to have the best performance; we plotted the importance of each variable in this model in order to determine the most important factors affecting price.

Finally, in order to map our selected model's performance in each neighborhood, we first added the predicted price values from the random forest model to the dataset for each listing. Then, we calculated the mean predicted price in each of the 235 distinct New York City neighborhoods in our dataset, along with the actual mean prices for each neighborhood, and calculated the error rate. Finally, we plotted all three of these measures for each listing by latitude and longitude, along with NYC's geographic boundaries.

As an aside, we'd like to mention a couple of approaches to this question we tried that did not work, or did not prove useful, and are thus not included below. They include:

-   Completing all the same analysis, but for occupancy rate: Our original question was about predicting New York's AirBnB occupancy rate, in addition to price. However, occupancy rate was not a variable we had access to in the dataset, so we sought to construct the occupancy rate for each listing. We realized that the algorithm that others who have studied AirBnB's occupancy rates have used (monthly reviews x expected review rate x average stay length) relied on the faulty assumptions that expected review rate and stay length are constant for all listings in the dataset. Rather than using an algorithm based on such faulty assumptions, we decided to forgo our analysis of occupancy rate and instead limit our analysis to price.

-   Topic Modeling: We tried using the topic modeling technique of Latent Dirichlet Allocation (LDA) to extract key words and topics from the names and descriptions of the listings. Unfortunately, upon performing this analysis, the topics were insufficiently differentiated from each other (e.g., four topics under the heading "apartment" in the top 10), and the coefficient of each topic was so low, that they yielded very little insight into predicted price and we excluded the results from this report. However, in a similar vein, we did pull out six key words from the amenities column and created new dummy variables to use as predictors in our models: pets_allowed, kitchen, heating, air_conditioning, elevator, and long_term_allowed.



 

## Results:

### Understanding Price

Price in our dataset is left-skewed; while some listings exceed \$1000 per night, 75% of listings are \$200 or less.

&nbsp;

```{r 3}
## Understanding Price
quartiles <- summary(filtered_data$price)
quartiles

# Create a histogram of price values
ggplot(filtered_data, aes(x = price)) +
  geom_histogram(binwidth = 10, color = "black", fill = "blue") +
  labs(title = "Histogram of Price Values (0 - 1500 USD)", x = "Price", y = "Frequency")
```

&nbsp;

Manhattan has the most expensive listings, with a median nightly price of \$160. Brooklyn follows at \$119 per night; listings in the other boroughs have median nightly prices between \$80 and \$100. Each borough has quite a few upper outliers in price, as demonstrated by the boxplot below.

&nbsp;

```{r 3.1}
# Price by neighborhood
median_prices_neighborhood <- filtered_data %>%
  group_by(neighbourhood_group_cleansed) %>%
  summarize(median_price = median(price))

kable(median_prices_neighborhood)

# Boxplot by neighborhood:
ggplot(filtered_data, aes(x = neighbourhood_group_cleansed, y = price)) +
  geom_boxplot(color = "black", fill = "blue") +
  labs(title = "Boxplot of Price by Neighborhood Group", 
       x = "Neighborhood Group", y = "Price")
```

&nbsp;

Below, we've included the median listing price across the city by room type and season. Hotel rooms are by far the most expensive, although they are the least common. Entire homes are more than twice as expensive as private rooms, and nearly three times as expensive as shared rooms.

```{r 3.2}
# Price by room type
median_prices_room_type <- NY_reduced %>%
  group_by(room_type) %>%
  summarize(median_price = median(price))

kable(median_prices_room_type)
```

&nbsp;

Season appears not to affect citywide median price much. The median price in each season ranges from \$120 in spring to \$130 in winter. Again, since we only have price data for one date per season, we lack the fine-grained price data that would be useful to identify bigger seasonal shifts, or price trends for each neighborhood (especially during e.g. big events or holidays). But for many listings in our dataset, the listed price was identical for each of the four dates of scraped data. It's possible that many AirBnB hosts don't change their prices much or at all from the default, although without better data this hypothesis is just speculation.

At the borough level, there is more seasonal variation in median price in some boroughs, and less in others. The range of Staten Island's median price by season is 25, while the range of Queens's median price by season is just 4.5.

&nbsp;

```{r 3.3}
# Price by season
# Compute the mean price for each season
median_price_by_season <- NY_reduced %>%
  gather(season, flag, spring:winter) %>%
  filter(flag == 1) %>%
  group_by(season) %>%
  summarise(median_price = median(price, na.rm = TRUE))

kable(median_price_by_season)

# Calculate median price by neighborhood per season
median_price_by_neighborhood_season <- NY_reduced %>%
  gather(season, flag, spring:winter) %>%
  filter(flag == 1) %>%
  group_by(neighbourhood_group_cleansed, season) %>%
  summarise(median_price = median(price, na.rm = TRUE))

# Print the result
kable(median_price_by_neighborhood_season)
```

 

### Building a Prediction Model

The table below gives the RMSE of each model we tested. The best-performing model is the random forest model, which has a RMSE of 0.326; on average, this model is roughly 32.6% off of the true price of a given listing.

&nbsp;

```{r 4}
## Predicting Price ($0-1500)
set.seed(2)
filtered_data_split = initial_split(filtered_data, prop=0.8)
filtered_data_train = training(filtered_data_split)
filtered_data_test  = testing(filtered_data_split)

lm_null <- lm(log_price ~ 1, data = filtered_data_train)

lm2 <- lm(log_price ~ latitude*longitude + review_scores_rating + bedrooms +
            neighbourhood_group_cleansed + entire_home + shared_room + private_room, 
          data = filtered_data_train)

lm3 <- lm(log_price ~ Pets_allowed + Kitchen + Elevator + Air_conditioning +
            Heating + Long_term_allowed, data = filtered_data_train)

#LASSO
# create your own numeric feature matrix.
x1 = sparse.model.matrix(log_price ~ .-1 - price - name - description - 
                           amenities - neighbourhood_cleansed, 
                         data=filtered_data)
y1 = filtered_data$price

# fit a single lasso
set.seed(2) # Set seed for reproducibility
lasso1 = gamlr(x1, y1, family="gaussian", penalty.factor=1)

# the coefficients at the AIC-optimizing value
scbeta1 = coef(lasso1)
scbeta1_nonzero <- which(scbeta1 != 0, arr.ind = TRUE)

# Linear Model, (some) LASSO features
lm_lasso1 <- lm(log_price ~ host_since + neighbourhood_group_cleansed + Kitchen +
                  accommodates + spatial_cluster + minimum_nights + Elevator + 
                  Air_conditioning + Heating + review_scores_rating + 
                  host_total_listings_count + bathrooms_text + host_identity_verified +
                  latitude * longitude + number_of_reviews + entire_home + shared_room +
                  reviews_per_month + review_scores_location + review_scores_value +
                  spring + fall, data = filtered_data_train)

# random tree
BNB.tree1 = rpart(log_price ~ spatial_cluster + host_since + 
                    neighbourhood_group_cleansed + host_total_listings_count +
                    accommodates + bedrooms + minimum_nights + availability_30 +
                    availability_90 + number_of_reviews + review_scores_cleanliness +
                    reviews_per_month + review_scores_location + review_scores_value +
                    review_scores_rating + instant_bookable + entire_home + 
                    shared_room + private_room + spring + summer + fall + winter,
                       data=filtered_data_train, control = rpart.control(cp = 0.00001))

# random forest
BNB.forest1 = randomForest(log_price ~ spatial_cluster + latitude*longitude + 
                             host_since + neighbourhood_group_cleansed +
                             Air_conditioning + Heating + Kitchen + Elevator +
                             host_total_listings_count + accommodates + bedrooms +
                             minimum_nights + availability_30 + availability_90 +
                             reviews_per_month + review_scores_location +
                             number_of_reviews + review_scores_cleanliness +
                             review_scores_value + review_scores_rating +
                             instant_bookable + entire_home + shared_room + 
                             private_room + spring + summer + fall + winter,
                           data=filtered_data_train, importance=TRUE)


# boosted model
BNB.boost1 = gbm(log_price ~ spatial_cluster + latitude*longitude + host_since +
                   host_total_listings_count + accommodates + bedrooms + 
                   Air_conditioning + Heating + Kitchen + Elevator +
                   minimum_nights + availability_30 + availability_90 +
                   review_scores_cleanliness + number_of_reviews + reviews_per_month +
                   review_scores_location + review_scores_value + review_scores_rating +
                   instant_bookable + entire_home + shared_room + private_room + 
                   spring + summer + fall + winter, 
                 data=filtered_data_train, distribution = "gaussian",
                 interaction.depth=6, n.trees=5000, shrinkage=.05, cv.folds = 2)

# Root mean squared error - log price
RMSE3 = tibble("model" = c("lm_null", "lm2", "lm3", "lm_lasso1", "BNB.tree1", 
                           "BNB.forest1", "BNB.boost1"),
               "RMSE" = c(rmse(lm_null, filtered_data_test), 
                          rmse(lm2, filtered_data_test), 
                          rmse(lm3, filtered_data_test), 
                          rmse(lm_lasso1, filtered_data_test), 
                          rmse(BNB.tree1, filtered_data_test), 
                          rmse(BNB.forest1, filtered_data_test), 
                          rmse(BNB.boost1, filtered_data_test)))
RMSE3.1 = kable(RMSE3, caption = "RMSE of each model (log price)")
RMSE3.1
```

&nbsp;

Below, we see two variable importance plots; the first represents the increase in node purity from each variable, and the second represents the percent increase in MSE if that variable is omitted. In general, the variables that have the most impact on the model are latitude & longitude (location), room type, minimum length of stay, number of guests the listing can accommodate, and reviews per month. The least important variables in our model are, in general, the seasonal variables and the amenity variables.

&nbsp;

```{r 4.1, fig.height=3.5}
# Variable Importance:
##varImpPlot(BNB.forest1)
imp <- importance(BNB.forest1)
imp_df <- data.frame(Var = rownames(imp), Importance = imp[, "IncNodePurity"], stringsAsFactors = FALSE)
long_imp <- reshape2::melt(imp_df, id.vars = "Var")

# Convert the importance data frame to a long format
long_imp <- reshape2::melt(imp_df, id.vars = "Var")

# Create the plot
clean_varimp_plot <- ggplot(long_imp, aes(x = reorder(Var, value), y = value, fill = Var)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  theme_minimal() +
  theme(axis.ticks.y = element_blank(),
        axis.title.y = element_blank(),
        legend.position = "none") +
  xlab("Variable") +
  ylab("Importance (IncNodePurity)")

# Show the plot
print(clean_varimp_plot)
```

&nbsp;

```{r 4.2, fig.height=3.5}
# Variable Importance:
##varImpPlot(BNB.forest1)
imp_df1 <- data.frame(Var = rownames(imp), Importance = imp[, "%IncMSE"], stringsAsFactors = FALSE)
long_imp1 <- reshape2::melt(imp_df1, id.vars = "Var")

# Create the plot
clean_varimp_plot1 <- ggplot(long_imp1, aes(x = reorder(Var, value), y = value, fill = Var)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  theme_minimal() +
  theme(axis.ticks.y = element_blank(),
        axis.title.y = element_blank(),
        legend.position = "none") +
  xlab("Variable") +
  ylab("Importance (%IncMSE)")

# Show the plot
print(clean_varimp_plot1)
```

 

### Mapping

```{r 6}
# Additional Data Manipulation
# Calculate the mean price per neighborhood
neighborhood_price <- filtered_data %>%
  group_by(neighbourhood_cleansed) %>%
  summarize(neighborhood_price = mean(price, na.rm = TRUE))

filtered_data <- merge(filtered_data, neighborhood_price, by = "neighbourhood_cleansed")

# get predicted values for all listings
forest1 = randomForest(log_price ~ spatial_cluster + latitude*longitude + 
                             host_since + neighbourhood_group_cleansed +
                             Air_conditioning + Heating + Kitchen + Elevator +
                             host_total_listings_count + accommodates + bedrooms +
                             minimum_nights + availability_30 + availability_90 +
                             reviews_per_month + review_scores_location +
                             number_of_reviews + review_scores_cleanliness +
                             review_scores_value + review_scores_rating +
                             instant_bookable + entire_home + shared_room + 
                             private_room + spring + summer + fall + winter,
                         data=filtered_data, importance=TRUE)

#add predictions to data
filtered_data = filtered_data %>%
  mutate(price_forest1_pred = predict(forest1))

# Calculate the mean predicted log price per neighborhood
neighborhood_pred_price <- filtered_data %>%
  group_by(neighbourhood_cleansed) %>%
  summarize(neighborhood_predicted_price = mean(price_forest1_pred, na.rm = TRUE))

# Merge the data frames by the common column 'neighbourhood_cleansed'
filtered_data <- merge(filtered_data, neighborhood_pred_price, 
                       by = "neighbourhood_cleansed")

# Convert back from log
filtered_data = filtered_data %>%
  mutate(neighborhood_pred_price_conv = exp(neighborhood_predicted_price))

# create log price per neighborhood
neighborhood_log_price <- filtered_data %>%
  group_by(neighbourhood_cleansed) %>%
  summarize(neighborhood_log_price = mean(log_price, na.rm = TRUE))

filtered_data <- merge(filtered_data, neighborhood_log_price, 
                       by = "neighbourhood_cleansed")

#create error measurements
filtered_data <- filtered_data %>%
  mutate("resid1" = abs(neighborhood_predicted_price - neighborhood_log_price))%>%
  mutate("percentErr1" = resid1/neighborhood_log_price)
```

Here we see the maps of true neighborhood (not borough; recall that, while there are 5 boroughs in NYC, there are over 250 neighborhoods in our dataset) as coded in the dataset, versus spatial cluster as predicted by the DBSCAN algorithm.

The spatial clusters do a reasonably good job of telling the difference between Manhattan, Brooklyn, and everywhere else, but they are not nearly as fine-grained as the true neighborhood from the dataset. This might explain why they are not a particularly important predictor of our random forest model.

&nbsp;

```{r 7, fig.height=4}
## Geography of Price
library(plotly)

filtered_data_sf <- st_as_sf(filtered_data, coords = c("longitude", "latitude"), 
                             crs = 4326)

geojson_file <- "/Users/jack/Documents/GitHub/Data-Mining-Statistical-Learning/Final Project/Borough_Boundaries.geojson"

# Read the GeoJSON file
nyc_shapefile <- st_read(geojson_file, quiet=TRUE)
nyc_shapefile2 <- nyc_shapefile %>%
  st_transform(crs = st_crs(filtered_data_sf))

## True Neighborhoods
plot1 <- ggplot() +
  geom_sf(data = nyc_shapefile2, fill = "lightgray", color = "black", size = 0.2) +
  geom_sf(data = filtered_data_sf, aes(color = neighbourhood_cleansed), size = .2) +
  theme_minimal() +
  labs(title = "True Neighborhoods",
       color = "Neighborhood") +
  theme(legend.position = "none")
plot1
```

&nbsp;

```{r 7.0.1, fig.height=4}
## Spatial Clusters
# Create the plot
plot2 <- ggplot() +
  geom_sf(data = nyc_shapefile2, fill = "lightgray", color = "black", size = 0.2) +
  geom_sf(data = filtered_data_sf, aes(color = ifelse(spatial_cluster == 0, "pink",
                                                      spatial_cluster)), size = .2) +
  theme_minimal() +
  labs(title = "Spatial Clusters (DBSCAN)",
       color = "Cluster (if not in cluster, listing is pink)") +
  theme(legend.position = "none")
plot2
```

 

Below, we see first the map of mean predicted price by neighborhood (converted back from log form for interpretability), followed by the map of actual mean price by neighborhood.

&nbsp;

```{r 7.1, fig.height=4}
## Mapping Price
# Predicted Price by Neighborhood
plot3 <- ggplot() +
  geom_sf(data = nyc_shapefile2, fill = "lightgray", color = "black", size = 0.2) +
  geom_sf(data = filtered_data_sf, aes(color = neighborhood_pred_price_conv), size = .2) +
  scale_color_gradientn(colours = viridisLite::viridis(10),
                        limits = c(0, 400),
                        name = "Pred. Price") +
  theme_minimal() +
  labs(title = "Predicted Price by Neighborhood",
       color = "Pred. Price")
plot3
```

&nbsp;

```{r 7.1.1, fig.height=4}
# True Price by Neighborhood
plot4 <- ggplot() +
  geom_sf(data = nyc_shapefile2, fill = "lightgray", color = "black", size = 0.2) +
  geom_sf(data = filtered_data_sf, aes(color = neighborhood_price), size = .2) +
  scale_color_gradientn(colours = viridisLite::viridis(10),
                        limits = c(0, 400),
                        name = "True Price") +
  theme_minimal() +
  labs(title = "True Price by Neighborhood",
       color = "True Price")
plot4
```

 

And below, we see the mean residual error rate of our model's predicted price in each neighborhood:

&nbsp;

```{r 7.2, fig.height=4}
# Residual Error of Price
ggplot() +
  geom_sf(data = nyc_shapefile2, fill = "lightgray", color = "black", size = 0.2) +
  geom_sf(data = filtered_data_sf, aes(color = 100*percentErr1), size = .2) +
  scale_color_gradientn(colours = viridisLite::plasma(10),
                        name = "Residual Error Rate") +
  theme_minimal() +
  labs(title = "Residual Error Rate by Neighborhood - Log Price",
       color = "Residual Error Rate")
```

 

## Conclusion:
*Interpret what you found. What are the main lessons we should take away from your report?*

Why was random forest the best model?

Why such a high RMSE?

What variables were most and least important to the best performing model?

What can hosts and travelers learn from those variables?

Where did our model perform well and where did it perform less well?





